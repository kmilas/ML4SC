{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'EnvCompatibility' from 'gymnasium.wrappers' (c:\\Users\\xskap\\miniconda3\\envs\\ml4sc\\Lib\\site-packages\\gymnasium\\wrappers\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 62\u001b[0m\n\u001b[0;32m     59\u001b[0m env \u001b[38;5;241m=\u001b[39m CustomRewardWrapper(base_env)\n\u001b[0;32m     61\u001b[0m \u001b[38;5;66;03m# Fix compatibility with SB3\u001b[39;00m\n\u001b[1;32m---> 62\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mgymnasium\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mwrappers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m EnvCompatibility\n\u001b[0;32m     63\u001b[0m env \u001b[38;5;241m=\u001b[39m EnvCompatibility(env)\n\u001b[0;32m     65\u001b[0m \u001b[38;5;66;03m# env = gym.make(env_id, render_mode='human')\u001b[39;00m\n\u001b[0;32m     66\u001b[0m \n\u001b[0;32m     67\u001b[0m \u001b[38;5;66;03m# model = A2C('MlpPolicy', env, verbose=1)\u001b[39;00m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'EnvCompatibility' from 'gymnasium.wrappers' (c:\\Users\\xskap\\miniconda3\\envs\\ml4sc\\Lib\\site-packages\\gymnasium\\wrappers\\__init__.py)"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import gym_unbalanced_disk, time\n",
    "import numpy as np\n",
    "from stable_baselines3 import PPO, A2C, SAC\n",
    "\n",
    "\n",
    "# env_id = \"CartPole-v1\"\n",
    "# env = gym.make(env_id)\n",
    "\n",
    "class CustomRewardWrapper(gym.Wrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "\n",
    "    def step(self, action):\n",
    "        obs, original_reward, terminated, truncated, info = self.env.step(action)\n",
    "        \n",
    "        # Extract angle and angular velocity from obs\n",
    "        theta = obs[0]  # angle\n",
    "        theta_dot = obs[1]  # angular velocity\n",
    "\n",
    "        # Define your custom reward\n",
    "        reward = - (theta**2 + 0.1 * theta_dot**2 + 0.01 * action**2)\n",
    "\n",
    "        return obs, reward, terminated, truncated, info\n",
    "    \n",
    "    def reset(self, seed=None, options=None):\n",
    "        if seed is not None:\n",
    "            super().reset(seed=seed)\n",
    "            self.np_random, _ = gym.utils.seeding.np_random(seed)\n",
    "        \n",
    "        # Reset my environment's state\n",
    "        obs = super().reset()\n",
    "        if isinstance(obs, tuple) and len(obs) == 2:\n",
    "            obs = obs[0]\n",
    "        # Return observation and empty info dict as expected by Gymnasium\n",
    "        return obs, {}\n",
    "\n",
    "base_env = gym_unbalanced_disk.UnbalancedDisk(dt=0.025, umax=3.)\n",
    "env = CustomRewardWrapper(base_env)\n",
    "\n",
    "# Fix compatibility with SB3\n",
    "from gymnasium.wrappers import EnvCompatibility\n",
    "env = EnvCompatibility(env)\n",
    "\n",
    "# env = gym.make(env_id, render_mode='human')\n",
    "\n",
    "# model = A2C('MlpPolicy', env, verbose=1)\n",
    "model = PPO('MlpPolicy', env, verbose=1) #b)\n",
    "model.learn(total_timesteps=25000)\n",
    "\n",
    "obs = env.reset()[0]\n",
    "try:\n",
    "    for _ in range(1000):\n",
    "        action, _states = model.predict(obs)\n",
    "        obs, reward, terminated, truncated, info = env.step(action)\n",
    "        env.render()\n",
    "        # time.sleep(1/50)\n",
    "        if terminated or truncated:\n",
    "            env.reset()[0]\n",
    "finally:\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 11\u001b[0m\n\u001b[0;32m      9\u001b[0m obs, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m eval_env\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[0;32m     10\u001b[0m total_reward \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward\n\u001b[1;32m---> 11\u001b[0m eval_env\u001b[38;5;241m.\u001b[39mrender()\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m terminated \u001b[38;5;129;01mor\u001b[39;00m truncated:\n\u001b[0;32m     13\u001b[0m     done \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\xskap\\miniconda3\\envs\\ml4sc\\Lib\\site-packages\\gym_unbalanced_disk\\envs\\UnbalancedDisk.py:136\u001b[0m, in \u001b[0;36mUnbalancedDisk.render\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    127\u001b[0m gfxdraw\u001b[38;5;241m.\u001b[39mfilled_circle( \u001b[38;5;66;03m#small nut\u001b[39;00m\n\u001b[0;32m    128\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msurf,\n\u001b[0;32m    129\u001b[0m     \u001b[38;5;28mint\u001b[39m(screen_width\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m2\u001b[39m\u001b[38;5;241m-\u001b[39msin(th)\u001b[38;5;241m*\u001b[39mr), \u001b[38;5;66;03m#is direction correct?\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    132\u001b[0m     (\u001b[38;5;241m71\u001b[39m,\u001b[38;5;241m63\u001b[39m,\u001b[38;5;241m48\u001b[39m),\n\u001b[0;32m    133\u001b[0m )\n\u001b[0;32m    135\u001b[0m fname \u001b[38;5;241m=\u001b[39m path\u001b[38;5;241m.\u001b[39mjoin(path\u001b[38;5;241m.\u001b[39mdirname(\u001b[38;5;18m__file__\u001b[39m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclockwise.png\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 136\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39marrow \u001b[38;5;241m=\u001b[39m pygame\u001b[38;5;241m.\u001b[39mimage\u001b[38;5;241m.\u001b[39mload(fname)\n\u001b[0;32m    137\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mu:\n\u001b[0;32m    138\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mu, (np\u001b[38;5;241m.\u001b[39mndarray,\u001b[38;5;28mlist\u001b[39m)):\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "eval_env = gym_unbalanced_disk.UnbalancedDisk(umax=3.0, dt=0.025, render_mode='human')\n",
    "num_eval_episodes = 5\n",
    "for ep in range(num_eval_episodes):\n",
    "    obs, _ = eval_env.reset()\n",
    "    done = False\n",
    "    total_reward = 0.0\n",
    "    while not done:\n",
    "        action, _states = model.predict(obs, deterministic=True)\n",
    "        obs, reward, terminated, truncated, info = eval_env.step(action)\n",
    "        total_reward += reward\n",
    "        eval_env.render()\n",
    "        if terminated or truncated:\n",
    "            done = True\n",
    "    print(f\"Episode {ep+1}: Total Reward = {total_reward:.3f}\")\n",
    "\n",
    "eval_env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml4sc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
