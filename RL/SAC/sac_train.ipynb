{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training of a simple SAC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import SAC\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.callbacks import EvalCallback, StopTrainingOnMaxEpisodes\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from gymnasium.wrappers import TimeLimit\n",
    "from gym_unbalanced_disk import UnbalancedDisk\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "\n",
    "# Training environment\n",
    "env = AC_UnbalancedDisk()\n",
    "env = TimeLimit(env, max_episode_steps=500)\n",
    "env = Monitor(env)\n",
    "\n",
    "# Separate eval environment for unbiased performance tracking\n",
    "eval_env = AC_UnbalancedDisk()\n",
    "eval_env = TimeLimit(eval_env, max_episode_steps=500)\n",
    "eval_env = Monitor(eval_env)\n",
    "\n",
    "# Stop after 100 episodes\n",
    "stop_cb = StopTrainingOnMaxEpisodes(max_episodes=100, verbose=1)\n",
    "\n",
    "# Save best model based on mean reward\n",
    "eval_cb = EvalCallback(\n",
    "    eval_env,\n",
    "    best_model_save_path=\"./best_sac_model\",\n",
    "    log_path=\"./logs\",\n",
    "    eval_freq=5000,\n",
    "    deterministic=True,\n",
    "    render=False\n",
    ")\n",
    "\n",
    "# Chain both callbacks\n",
    "from stable_baselines3.common.callbacks import CallbackList\n",
    "callback = CallbackList([stop_cb, eval_cb])\n",
    "\n",
    "# Model\n",
    "model_sac1 = SAC(\n",
    "    policy='MlpPolicy',\n",
    "    env=env,\n",
    "    learning_rate=1e-3,\n",
    "    verbose=1,\n",
    "    ent_coef=1e-2,\n",
    ")\n",
    "\n",
    "# Train\n",
    "model_sac1.learn(\n",
    "    total_timesteps=1_000_000,\n",
    "    callback=callback,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To use the best-performing model\n",
    "from stable_baselines3 import SAC\n",
    "model_sac1 = SAC.load(\"./best_sac_model/best_model.zip\")\n",
    "\n",
    "\n",
    "env = AC_UnbalancedDisk()\n",
    "obs, _ = env.reset()\n",
    "for i in range(5000):\n",
    "    action, _states = model_sac1.predict(obs)  # policy\n",
    "    obs, reward, terminated, truncated, info = env.step(action)\n",
    "    env.render()\n",
    "    t = obs[0]\n",
    "    print( f'theta = {t: .4f}, omega: {obs[1]: .4f}')\n",
    "    if terminated or truncated:\n",
    "        obs, _ = env.reset()\n",
    "    \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training of multiple SACs with Optuna (A run for over 400 minutes expected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "from optuna.pruners import MedianPruner\n",
    "from optuna.samplers import TPESampler\n",
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "from stable_baselines3 import SAC\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.callbacks import EvalCallback, StopTrainingOnMaxEpisodes, CallbackList\n",
    "from gymnasium.wrappers import TimeLimit\n",
    "\n",
    "def objective(trial):\n",
    "\n",
    "    # Sample hyperparameters\n",
    "    learning_rate = trial.suggest_float(\"learning_rate\", 1e-5, 1e-3, log=True)\n",
    "    # ent_coef = trial.suggest_float(\"ent_coef\", 1e-5, 0.1, log=True)\n",
    "    gamma = trial.suggest_float(\"gamma\", 0.90, 0.9999)\n",
    "    tau = trial.suggest_float(\"tau\", 0.005, 0.02)\n",
    "    batch_size = trial.suggest_categorical(\"batch_size\", [64, 128, 256])\n",
    "    buffer_size = trial.suggest_categorical(\"buffer_size\", [100_000, 200_000, 500_000])\n",
    "\n",
    "    # Training env\n",
    "    env = Monitor(TimeLimit(AC_UnbalancedDisk1(randomize_friction=True), max_episode_steps=500))\n",
    "\n",
    "    # Evaluation env (no random friction)\n",
    "    eval_env = Monitor(TimeLimit(AC_UnbalancedDisk1(randomize_friction=False), max_episode_steps=500))\n",
    "\n",
    "    # Callbacks\n",
    "    stop_cb = StopTrainingOnMaxEpisodes(max_episodes=100, verbose=1)\n",
    "    eval_cb = EvalCallback(\n",
    "        eval_env,\n",
    "        best_model_save_path=f\"./optuna_best_model_trial_{trial.number}\",\n",
    "        log_path=None,\n",
    "        eval_freq=5000,\n",
    "        deterministic=True,\n",
    "        render=False,\n",
    "    )\n",
    "    callback = CallbackList([stop_cb, eval_cb])\n",
    "\n",
    "    # Model\n",
    "    model = SAC(\n",
    "        \"MlpPolicy\",\n",
    "        env,\n",
    "        learning_rate=learning_rate,\n",
    "        # ent_coef=ent_coef,\n",
    "        gamma=gamma,\n",
    "        tau=tau,\n",
    "        batch_size=batch_size,\n",
    "        buffer_size=buffer_size,\n",
    "        verbose=1,\n",
    "        seed=42,\n",
    "    )\n",
    "\n",
    "    model.learn(total_timesteps=1_000_000, callback=callback)\n",
    "\n",
    "    return eval_cb.best_mean_reward\n",
    "\n",
    "# Optuna study\n",
    "study = optuna.create_study(direction=\"maximize\", sampler=TPESampler(), pruner=MedianPruner())\n",
    "study.optimize(objective, n_trials=20)\n",
    "\n",
    "# Show best result\n",
    "print(\"Best trial value:\", study.best_trial.value)\n",
    "print(\"Best hyperparameters:\", study.best_trial.params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To use the best-performing model\n",
    "from stable_baselines3 import SAC\n",
    "model = SAC.load(f\"./optuna_best_model_trial_{17}/best_model.zip\")\n",
    "\n",
    "\n",
    "\n",
    "env = AC_UnbalancedDisk1()\n",
    "obs, _ = env.reset()\n",
    "for i in range(1000):\n",
    "    action, _states = model.predict(obs)  # policy\n",
    "    obs, reward, terminated, truncated, info = env.step(action)\n",
    "    env.render()\n",
    "    t = obs[0]\n",
    "    print( f'theta = {t: .4f}, omega: {obs[1]: .4f}')\n",
    "    if terminated or truncated:\n",
    "        obs, _ = env.reset()\n",
    "    \n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
