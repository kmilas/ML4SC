{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import wandb\n",
    "import time\n",
    "import torch\n",
    "from torch import nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from matplotlib import pyplot as plt "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = np.load('training-val-test-data.npz')\n",
    "th_train = out['th']\n",
    "u_train = out['u']\n",
    "\n",
    "def create_IO_data(u,y,na,nb):\n",
    "    X = []\n",
    "    Y = []\n",
    "    for k in range(max(na,nb), len(y)):\n",
    "        X.append(np.concatenate([u[k-nb:k],y[k-na:k]]))\n",
    "        Y.append(y[k])\n",
    "    return np.array(X), np.array(Y)\n",
    "\n",
    "na = 11\n",
    "nb = 10\n",
    "X, Y = create_IO_data(u_train, th_train, na, nb)\n",
    "\n",
    "# Split into train/val/test\n",
    "Xtemp, Xtest_final, Ytemp, Ytest_final = train_test_split(X, Y, test_size=0.15, random_state=42)\n",
    "Xtrain_final, Xval, Ytrain_final, Yval = train_test_split(Xtemp, Ytemp, test_size=0.1765, random_state=42)\n",
    "Xtrain_final = np.expand_dims(Xtrain_final, axis=-1)\n",
    "Xval = np.expand_dims(Xval, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(21, 1)\n"
     ]
    }
   ],
   "source": [
    "print(Xtrain_final[0].shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NOELSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
    "        super(NOELSTM, self).__init__()\n",
    "        # self.hidden_size = hidden_size\n",
    "        # self.num_layers = num_layers\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True).double()\n",
    "        self.fc = nn.Linear(hidden_size, output_size).double()\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        out, _ = self.lstm(x)            # out: (batch_size, seq_len, hidden_size)\n",
    "        out = self.fc(out[:, -1, :])     # Take output from the last time step\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\xskap\\Documents\\Eindhoven\\2025_Q4\\5SC28_ML_systems_control\\Assignment\\wandb\\run-20250521_120748-a9oxx5dk</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/skapetis-tu-e/5SC28/runs/a9oxx5dk' target=\"_blank\">LSTM 32 size 1 layers n = 11-10</a></strong> to <a href='https://wandb.ai/skapetis-tu-e/5SC28' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/skapetis-tu-e/5SC28' target=\"_blank\">https://wandb.ai/skapetis-tu-e/5SC28</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/skapetis-tu-e/5SC28/runs/a9oxx5dk' target=\"_blank\">https://wandb.ai/skapetis-tu-e/5SC28/runs/a9oxx5dk</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Epoch 0/1000 | Train NRMS: 0.46251 | Val NRMS: 0.47640 | LR: 0.000000\n",
      "1.29 updates/sec\n",
      "Epoch 100/1000 | Train NRMS: 0.46089 | Val NRMS: 0.47483 | LR: 0.000000\n",
      "1.53 updates/sec\n",
      "Epoch 200/1000 | Train NRMS: 0.46089 | Val NRMS: 0.47483 | LR: 0.000000\n",
      "1.45 updates/sec\n",
      "Epoch 300/1000 | Train NRMS: 0.46089 | Val NRMS: 0.47483 | LR: 0.000000\n",
      "1.43 updates/sec\n",
      "Epoch 400/1000 | Train NRMS: 0.46089 | Val NRMS: 0.47483 | LR: 0.000000\n",
      "1.43 updates/sec\n",
      "Epoch 500/1000 | Train NRMS: 0.46089 | Val NRMS: 0.47483 | LR: 0.000000\n",
      "1.46 updates/sec\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[39], line 75\u001b[0m\n\u001b[0;32m     73\u001b[0m y \u001b[38;5;241m=\u001b[39m model(Xtrain_tensor[i:i \u001b[38;5;241m+\u001b[39m wandb\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mbatch_size])\n\u001b[0;32m     74\u001b[0m loss \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmean((y \u001b[38;5;241m-\u001b[39m Ytrain_tensor[i:i \u001b[38;5;241m+\u001b[39m wandb\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mbatch_size]) \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m---> 75\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     76\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     77\u001b[0m batch_grads \u001b[38;5;241m=\u001b[39m [torch\u001b[38;5;241m.\u001b[39mmean(par\u001b[38;5;241m.\u001b[39mgrad\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m0.5\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m par \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(model\u001b[38;5;241m.\u001b[39mparameters())[::\u001b[38;5;241m2\u001b[39m]]\n",
      "File \u001b[1;32mc:\\Users\\xskap\\miniconda3\\envs\\ml4sc\\Lib\\site-packages\\torch\\_tensor.py:648\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    638\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    639\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    640\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    641\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    646\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    647\u001b[0m     )\n\u001b[1;32m--> 648\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[0;32m    649\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[0;32m    650\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\xskap\\miniconda3\\envs\\ml4sc\\Lib\\site-packages\\torch\\autograd\\__init__.py:353\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    348\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    350\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    351\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    352\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 353\u001b[0m _engine_run_backward(\n\u001b[0;32m    354\u001b[0m     tensors,\n\u001b[0;32m    355\u001b[0m     grad_tensors_,\n\u001b[0;32m    356\u001b[0m     retain_graph,\n\u001b[0;32m    357\u001b[0m     create_graph,\n\u001b[0;32m    358\u001b[0m     inputs,\n\u001b[0;32m    359\u001b[0m     allow_unreachable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    360\u001b[0m     accumulate_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    361\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\xskap\\miniconda3\\envs\\ml4sc\\Lib\\site-packages\\torch\\autograd\\graph.py:824\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    822\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    823\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 824\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    825\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    826\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    827\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    828\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Initialize W&B\n",
    "wandb.init(\n",
    "    project=\"5SC28\",\n",
    "    name=\"LSTM 32 size 1 layers n = 11-10\",\n",
    "    config={\n",
    "        \"input_size\": 1,\n",
    "        \"hidden_size\":32,\n",
    "        \"num_layers\":1,\n",
    "        \"output_size\":1,\n",
    "        \"learning_rate\": 0.01,\n",
    "        \"epochs\": 1000,\n",
    "        \"batch_size\": 256,\n",
    "        \"batched\": True,\n",
    "        \"lr_decay\": \"plateau\",         # options: \"step\", \"plateau\"        \n",
    "        \"lr_gamma\": 0.5,\n",
    "        \"lr_step_size\": 500\n",
    "    }\n",
    ")\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "train_NRMS_list = []\n",
    "val_NRMS_list = []\n",
    "\n",
    "# Set up device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Model setup\n",
    "model = NOELSTM(wandb.config.input_size, wandb.config.hidden_size, wandb.config.num_layers, wandb.config.output_size).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=wandb.config.learning_rate)\n",
    "wandb.watch(model, log=\"all\")\n",
    "\n",
    "# Learning rate scheduler\n",
    "if wandb.config.lr_decay == \"step\":\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(\n",
    "        optimizer,\n",
    "        step_size=wandb.config.lr_step_size,\n",
    "        gamma=wandb.config.lr_gamma\n",
    "    )\n",
    "elif wandb.config.lr_decay == \"plateau\":\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer,\n",
    "        mode='min',\n",
    "        factor=wandb.config.lr_gamma,\n",
    "        patience=20,\n",
    "    )\n",
    "else:\n",
    "    scheduler = None\n",
    "\n",
    "# Convert to torch tensors\n",
    "Xtrain_tensor = torch.tensor(Xtrain_final).to(device)\n",
    "Ytrain_tensor = torch.tensor(Ytrain_final).to(device)\n",
    "Xval_tensor = torch.tensor(Xval).to(device)\n",
    "Yval_tensor = torch.tensor(Yval).to(device)\n",
    "Xtest_tensor = torch.tensor(Xtest_final).to(device)\n",
    "Ytest_tensor = torch.tensor(Ytest_final).to(device)\n",
    "\n",
    "# y_mean = y_tensor.mean()\n",
    "# y_std = y_tensor.std()\n",
    "# u_tensor = (u_tensor - u_tensor.mean()) / u_tensor.std()\n",
    "# y_tensor = (y_tensor - y_mean) / y_std\n",
    "\n",
    "grads = []\n",
    "t_start = time.time()\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(wandb.config.epochs):\n",
    "    model.train()\n",
    "    if wandb.config.batched:\n",
    "        for i in range(0, len(Xtrain_tensor) - wandb.config.batch_size, wandb.config.batch_size):\n",
    "            optimizer.zero_grad()\n",
    "            y = model(Xtrain_tensor[i:i + wandb.config.batch_size])\n",
    "            loss = torch.mean((y - Ytrain_tensor[i:i + wandb.config.batch_size]) ** 2)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            batch_grads = [torch.mean(par.grad**2).item()**0.5 for par in list(model.parameters())[::2]]\n",
    "            grads.append(batch_grads)\n",
    "    else:\n",
    "        optimizer.zero_grad()\n",
    "        y = model(Xtrain_tensor)\n",
    "        loss = torch.mean((y - Ytrain_tensor) ** 2)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        batch_grads = [torch.mean(par.grad**2).item()**0.5 for par in list(model.parameters())[::2]]\n",
    "        grads.append(batch_grads)\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_pred = model(Xval_tensor)\n",
    "        val_loss = torch.mean((val_pred - Yval_tensor) ** 2)\n",
    "        val_nrms = val_loss.item() / torch.std(Yval_tensor).item()\n",
    "\n",
    "    # # Compute NRMS\n",
    "    # NRMS = Loss.item() / torch.std(y_tensor).item()\n",
    "\n",
    "    # if scheduler:\n",
    "    #     if wandb.config.lr_decay == \"plateau\":\n",
    "    #         scheduler.step(val_loss.item())\n",
    "    #     else:\n",
    "    #         scheduler.step()\n",
    "    #     current_lr = scheduler.get_last_lr()[0]\n",
    "    # else:\n",
    "    #     current_lr = wandb.config.learning_rate\n",
    "\n",
    "    train_nrms = loss.item() / torch.std(Ytrain_tensor).item()\n",
    "\n",
    "    train_losses.append(loss.item())\n",
    "    val_losses.append(val_loss.item())\n",
    "    train_NRMS_list.append(train_nrms)\n",
    "    val_NRMS_list.append(val_nrms)\n",
    "\n",
    "    wandb.log({\n",
    "        \"epoch\": epoch,\n",
    "        \"train_loss\": loss.item(),\n",
    "        \"train_NRMS\": train_nrms,\n",
    "        \"val_loss\": val_loss.item(),\n",
    "        \"val_NRMS\": val_nrms,\n",
    "        \"updates_per_sec\": (epoch + 1) / (time.time() - t_start),\n",
    "        \"learning_rate\": wandb.config.learning_rate\n",
    "    })\n",
    "\n",
    "    # Print progress\n",
    "    if epoch % 100 == 0:\n",
    "        print(f\"Epoch {epoch}/{wandb.config.epochs} | Train NRMS: {train_nrms:.5f} | Val NRMS: {val_nrms:.5f} | LR: {current_lr:.6f}\")\n",
    "        print(f\"{(epoch + 1) / (time.time() - t_start):.2f} updates/sec\")\n",
    "\n",
    "# Save model\n",
    "# torch.save(model.state_dict(), \"model_val.pt\")\n",
    "# wandb.save(\"model_val.pt\")\n",
    "\n",
    "# Finish W&B session\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇▇▇██</td></tr><tr><td>learning_rate</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_NRMS</td><td>█▄▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss</td><td>█▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>updates_per_sec</td><td>▇███▇▄▃▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_NRMS</td><td>▁▂▅█▂▂▂▃▃▄▄▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅</td></tr><tr><td>val_loss</td><td>█▆▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>546</td></tr><tr><td>learning_rate</td><td>0.01</td></tr><tr><td>train_NRMS</td><td>0.46089</td></tr><tr><td>train_loss</td><td>0.22115</td></tr><tr><td>updates_per_sec</td><td>1.4506</td></tr><tr><td>val_NRMS</td><td>0.47483</td></tr><tr><td>val_loss</td><td>0.22541</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">LSTM 32 size 1 layers n = 11-10</strong> at: <a href='https://wandb.ai/skapetis-tu-e/5SC28/runs/a9oxx5dk' target=\"_blank\">https://wandb.ai/skapetis-tu-e/5SC28/runs/a9oxx5dk</a><br> View project at: <a href='https://wandb.ai/skapetis-tu-e/5SC28' target=\"_blank\">https://wandb.ai/skapetis-tu-e/5SC28</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250521_120748-a9oxx5dk\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml4sc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
