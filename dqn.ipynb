{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c13955f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from gym_unbalanced_disk import UnbalancedDisk\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "\n",
    "class AC_UnbalancedDisk(UnbalancedDisk):\n",
    "    def __init__(self, umax=3., dt=0.025, render_mode='human'):\n",
    "        super().__init__(umax=umax, dt=dt, render_mode=render_mode)\n",
    "\n",
    "        self.target = np.pi\n",
    "        low = [-np.pi, -40]\n",
    "        high = [np.pi, 40]\n",
    "        self.observation_space = spaces.Box(\n",
    "            low=np.array(low, dtype=np.float32),\n",
    "            high=np.array(high, dtype=np.float32),\n",
    "            shape=(2,)\n",
    "        )\n",
    "\n",
    "        self.recent_omegas = []\n",
    "\n",
    "    def step(self, action):\n",
    "        obs, reward, terminated, truncated, info = super().step(action)\n",
    "\n",
    "        th = obs[0]\n",
    "        omega = obs[1]\n",
    "\n",
    "        # Normalize angle so Ï€ maps to 0\n",
    "        theta = ((th - np.pi) % (2 * np.pi)) - np.pi\n",
    "\n",
    "        # Update buffer of recent omega values\n",
    "        self.recent_omegas.append(omega)\n",
    "        if len(self.recent_omegas) > 10:\n",
    "            self.recent_omegas.pop(0)\n",
    "\n",
    "        # Base reward structure\n",
    "        if abs(theta) < np.pi / 2:\n",
    "            reward = min(-0.5, -5 + abs(omega))\n",
    "        elif abs(theta) > np.pi / 2 and abs(theta) < 3 * np.pi / 4:\n",
    "            reward = abs(theta)**2 / (1 + abs(omega))**1\n",
    "        elif abs(theta) > 3 * np.pi / 4 and abs(theta) < 11 * np.pi / 12:\n",
    "            reward = abs(theta)**4 / (1 + abs(omega))**2\n",
    "\n",
    "            # Add anti-stall penalty\n",
    "            # Stall detection: angular velocity near zero for several steps\n",
    "            if all(abs(w) < 0.005 for w in self.recent_omegas):\n",
    "                reward = 1 / (1 + abs(omega))\n",
    "        else:\n",
    "            reward = abs(theta)**4 / (1 + abs(omega))**2\n",
    "             \n",
    "\n",
    "        return obs, reward, terminated, truncated, info\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        obs, info = super().reset()\n",
    "        self.recent_omegas = []  # Reset history\n",
    "        return obs, info\n",
    "\n",
    "\n",
    "class DQN_UnbalancedDisk(AC_UnbalancedDisk):\n",
    "    def __init__(self, umax=3., dt=0.025, n_actions=10, render_mode='human'):\n",
    "        super().__init__(umax=umax, dt=dt, render_mode=render_mode)\n",
    "\n",
    "\n",
    "        self.actions = np.linspace(-umax, umax, n_actions)\n",
    "\n",
    "        # Override action space to Discrete\n",
    "        self.action_space = spaces.Discrete(n_actions)\n",
    "\n",
    "    def step(self, action):\n",
    "        idx = int(np.argmin(np.abs(self.actions - action)))\n",
    "        # Build the 1-D array the parent expects\n",
    "        discrete_action = np.array([self.torques[idx]], dtype=np.float32)\n",
    "        obs, reward, terminated, truncated, info = super().step(discrete_action)\n",
    "\n",
    "        return obs, reward, terminated, truncated, info\n",
    "\n",
    "    #def reset(self, seed=None, options=None):\n",
    "    #    obs, info = super().reset()\n",
    "    #    self.recent_omegas = []  # Reset history\n",
    "    #    return obs, info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "548a9e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import DQN\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from gymnasium.wrappers import TimeLimit\n",
    "\n",
    "# Create and wrap your env\n",
    "env = DQN_UnbalancedDisk(umax=3.0, dt=0.025)\n",
    "env = TimeLimit(env, max_episode_steps=500)\n",
    "env = Monitor(env)\n",
    "\n",
    "# Instantiate and train DQN\n",
    "model_dqn = DQN(\n",
    "    policy=\"MlpPolicy\",\n",
    "    env=env,\n",
    "    learning_rate=1e-3,\n",
    "    buffer_size=50_000,\n",
    "    learning_starts=1_000,\n",
    "    batch_size=64,\n",
    "    tau=1.0,\n",
    "    gamma=0.99,\n",
    "    train_freq=4,\n",
    "    target_update_interval=1_000,\n",
    "    verbose=1,\n",
    ")\n",
    "model_dqn.learn(total_timesteps=500_000)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml4sc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
